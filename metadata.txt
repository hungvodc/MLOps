Supervised learning is the machine learning task of learning a function that maps an input to an output based on input-output example pairs. It infers a function from labeled training data consisting of a set of training examples. Each example is a pair consisting of an input object and a desired output value. A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario allows for the algorithm to correctly determine the class labels for unseen instances. Supervised learning problems are categorized into regression and classification problems. In supervised learning, each example is a pair consisting of an input object and a desired output value.
Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data. Clustering algorithms are used to classify objects or data into groups called clusters, such that objects within the same cluster are more similar to each other than objects belonging to different clusters. Unlike supervised classification, clustering analysis does not rely on predefined classes. Rather, similar groups of data are clustered together using statistical algorithms that find similarities among data instances. Unsupervised learning problems further include dimensionality reduction and association rule learning.
Reinforcement learning is a machine learning training technique concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning algorithms are used in autonomous vehicles, robotics, game playing, and numerous other application areas. In reinforcement learning, the agent learns from the environment through trial and error interactions. The environment provides reward or penalty for each action that the agent takes. The agent tries various strategies called policies to maximize the long term reward. The agent needs to balance exploration of uncharted territory with exploitation of current knowledge. Too much exploration may lead to too little exploitation. Reinforcement learning differs from supervised learning in not needing labeled input/output pairs, and in not telling the agent exactly what action to take, but instead relying on rewards or punishments to shape behavior.
Classification is the machine learning task of predicting the categorical class label of new instances based on past observations. A classification algorithm takes data with known labels as input and builds a model to predict the class of new unlabeled data. For example, an algorithm that classifies email messages as spam or not spam is a classification algorithm. Popular classification algorithms include logistic regression, decision trees, support vector machines and random forests.
Clustering is an unsupervised machine learning technique used to group data points together that have similar characteristics. The goal of clustering is to find groups such that data points in the same group are more similar to each other than points in different groups. Unlike classification, clustering does not rely on predefined classes. Popular clustering algorithms include k-means, hierarchical clustering, Gaussian mixture models and DBSCAN. Clustering requires defining a similarity measure between data points, like Euclidean distance or cosine similarity. Performance metrics include intra-cluster variance, Silhouette coefficient and Calinski-Harabasz index. Applications include market segmentation, social network analysis, recommendation systems and medical imaging analysis.
Dimensionality reduction is the process of reducing the number of variables under consideration by obtaining a set of principal variables. It helps reduce overfitting, improves model performance, and reduces training time. Methods like principal component analysis use orthogonal transformation to convert possibly correlated variables into linearly uncorrelated variables called principal components. Linear discriminant analysis finds linear combinations of features that separate classes. Both convert data in higher dimensions to lower dimensions.
Overfitting refers to a model that models the training data too well, capturing the noise and details, that it negatively impacts its ability to generalize to new data. It occurs when a model fits the training data too closely. An overfit model has low bias but very high variance. It has memorized the noise and details in the training set to the extent that it negatively impacts its performance on new data. This leads to poor predictions on unseen data though it may have high performance on training data. Overfitting can be mitigated by techniques like regularization, early stopping, and dropout.
Gradient descent is an optimization algorithm that minimizes loss by iteratively moving in the direction of steepest descent. The slope of the surface dictates the direction of movement. It follows the direction of decreasing objective function gradient to find the local minimum. For machine learning, gradient descent tweaks model parameters like weights and biases of neural networks or coefficients of regression models to minimize loss. The learning rate determines the size of steps to reach a local minimum. Adaptations like momentum, Nesterov accelerated gradient keep the optimization stable.
Random forests or random decision forests are an ensemble learning method that operate by constructing a multitude of decision trees at training time. For each tree in the ensemble, a random sample is drawn from the training set when growing the tree. While splitting each node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. This process of randomizing helps reduce overfitting across a large number of trees and improves generalization ability. The output prediction is made by each tree in the forest, with the classification being the mode of the classes output by individual trees.
K-nearest neighbors or KNN algorithm is a simple supervised machine learning algorithm that can be used for both classification and regression tasks. It assumes that data points that are in close proximity likely have similar properties and class labels. KNN stores all training data and classifies new data points based on majority class of its k closest neighbors in the training set. A data point is assigned the class label that appears most frequently among its k nearest neighbors. The distance metric commonly used is Euclidean distance. The value of k is a positive integer and varies based on nature of input data. KNN makes no assumptions about underlying data and chooses neighbors based on distance functions.
Kernel methods refer to a class of algorithms that depends on the data only through dot products. The kernel represents a function that quantifies similarity between two data points. It transforms data into a higher dimensional space to make it separable. Kernel functions like radial basis function and polynomial kernels correspond to similarity measures used by algorithms like support vector machines for classification and regression. Kernel trick is used to avoid explicitly mapping data to higher dimensions, instead using the kernel function to compute dot product between mapped data points. This makes computation faster. Kernel methods allow generalization of linear algorithms to nonlinear settings.
Cross-entropy loss, or log loss, measures difference between two probability distributions for use in classification problems with probabilities as output. It uses a probabilistic interpretation to logistic regression and outputs a probability between 0 and 1 for binary classification. The loss function measures dissimilarity between true labels and predicted probabilities. It increases as predicted probability diverges from true label. Log loss penalizes false classifications more than correct ones and rewards high confidence predictions that are correct. It is useful for multi-class prediction as well by applying softmax to neural network output to represent class probabilities.
The bias-variance tradeoff refers to balancing the bias and variance of a model to optimize its performance. Bias represents error due to oversimplifying the model. High bias can cause underfitting. Variance represents sensitivity to changes in training data. High variance can cause overfitting. Ideally, we want low bias and low variance models. As model complexity increases, variance typically increases and bias decreases. Tuning model hyperparameters involves balancing bias and variance. Simple models have high bias, low variance. Complex models have low bias, high variance.
Regularization refers to adding a penalty term to the loss function to induce a model that fits training data well but does not overfit. The penalty term constraints model parameters that lead to complex models. Regularization helps improve generalization by preventing overfitting. Two common types of regularization are L1 and L2 regularization. L1 adds penalty equivalent to absolute value of magnitude of coefficients. L2 adds penalty equivalent to square of magnitude of coefficients. This shrinks coefficients and makes model simpler and less prone to overfitting.
Support vector machines or SVM are supervised learning models used for classification and regression tasks. SVMs find the optimal hyperplane in an N-dimensional space that distinctly separates the data points into classes. The hyperplane maximizes the margin between the two classes. The data points nearest the hyperplane are support vectors. Kernel functions like polynomial or RBF are used to transform data to higher dimensions to make it separable. SVM handles nonlinear relationships well. SVMs can be extended to multiple classes using one-vs-one or one-vs-rest approaches. Parameters like soft margin and regularization affect performance.
A confusion matrix visually summarizes the performance of a classification model. It is a table layout that allows visualization of the performance of an algorithm. Each row represents instances of the actual class, while each column represents instances of the predicted class. It provides the counts of true positives, true negatives, false positives and false negatives. Additional evaluation metrics like accuracy, precision, recall and F1-score can be derived from the confusion matrix. It provides insight into errors being made by a classifier and types of misclassifications.
K-fold cross-validation is a resampling procedure used to evaluate machine learning models on a limited dataset. The procedure has a single parameter called k that refers to the number of groups that the given data is to be split into. The dataset is divided into k subsets of equal size. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set. The model is fit on the training set and evaluated on the test set. The procedure is repeated k times so that each subset serves as the test set once. The k results are averaged to produce a single estimation. The advantage of k-fold cross-validation is that all observations are used for both training and testing.
Feature selection is the process of selecting a subset of relevant features in the data that contribute most to the prediction variable or output in which we are interested. It helps reduce overfitting, improves model performance, and reduces training time. Methods like filter methods select subsets of variables independent of the model. Wrapper methods utilize the model performance to select features. Embedded methods learn which features contribute most to the accuracy of the model while training. The key goals are to improve model accuracy, reduce overfitting, reduce training time, and provide faster and more cost-effective models. Common approaches include univariate selection, recursive feature elimination, and regularization.
Boosting refers to an ensemble technique that converts weak learners into strong learners for classification or regression problems. It focuses on training data that was incorrectly predicted by previous models and gives higher priority to examples that were misclassified. AdaBoost is a commonly used boosting technique. It trains subsequent weak models on modified versions of the training data that focus more on examples misclassified by previous models. The final model prediction is calculated as a weighted sum of the predictions from all the trained weak models. Boosting helps reduce bias and variance to improve model performance. The general idea is to train models sequentially, with each new model attempting to correct errors from the previous model.
Precision is a performance metric for classification models. It refers to the number of positive class predictions made by the model that actually belong to the positive class divided by the total number of positive class predictions made. It provides insights into how many selected items are relevant. High precision indicates lower number of false positives. High recall and high precision indicates a good classifier. Precision is also referred to as positive predictive value.
Naive Bayes classifier is a simple probabilistic classifier based on Bayes' theorem that assumes strong independence between features. It is termed naive because it makes the simplifying assumption that the presence of a feature is unrelated to the presence of any other feature. This conditional independence assumption allows the model to estimate the probability of a class given an instance by the probabilities of the attributes present in the instance. Even when the assumption of independence is violated, the model performs surprisingly well in most cases. Naive Bayes models are easy to build and useful for very high-dimensional datasets. They often outperform even highly sophisticated models.
Hyperparameter tuning refers to choosing optimal hyperparameters of machine learning algorithms to improve model performance. Hyperparameters are parameters whose values are set prior to model training, unlike model parameters that are learned during training. For example, for kNN model, k is a hyperparameter. Grid search evaluates performance for all hyperparameter combinations in a specified grid. Random search samples combinations randomly from a hyperparameter space. Bayesian methods use probabilistic models for hyperparameter optimization. The optimal combination is found by iterative optimization guided by a validation set performance metric like accuracy. Automated tuning removes manual effort to find the best hyperparameters.
Decision trees are a type of supervised learning algorithm used for both regression and classification. They create a model that predicts the value or class of a target variable by learning simple decision rules inferred from the data features. It is called a tree because it starts with a root node and branches into possible decisions that lead to terminal nodes or leaf nodes that represent the outcome. Decision trees learn by a process of recursive partitioning of the data. Their advantages include interpretability, ability to handle nonlinear relationships and few tuning parameters.
Logistic regression is an algorithm for classification problems when the output variable is categorical, for example binary classification problems with two possible classes. Logistic regression uses a logistic function with a logit link to model the probability of the binary outcomes. The probabilities are mapped to two classes by applying a threshold, usually 0.5. The model coefficients are learned using maximum likelihood estimation. Logistic loss function measures probability error and is minimized to train the coefficients. Regularization like L1 and L2 helps prevent overfitting. Logistic regression assumes linear relationship between log odds of outcomes and predictor variables.
ROC curve stands for Receiver Operating Characteristic curve. It is a plot of the true positive rate vs the false positive rate for a classification model as the discrimination threshold varies. It visualizes the tradeoff between true positives and false positives. The true positive rate is also known as recall. The false positive rate is one minus specificity. Area under the ROC curve or AUC summarizes model performance. Higher AUC indicates better classification ability of the model. ROC analysis provides techniques to select optimal models and discard suboptimal ones independently of class distribution or error costs.
Ensemble learning refers to combining multiple machine learning models together to improve overall predictive performance. Popular ensemble techniques include boosting, bagging and stacking. Bagging trains models on different subsets of data in parallel. Boosting trains models sequentially. Stacking trains a meta-model on top of other models. Ensemble methods often reduce variance, bias and improve predictions. They combine outputs of individual models by techniques like majority voting for classification or averaging for regression. Ensembles can also combine models built using different algorithms. Model diversity is key for ensemble performance.
Random search is a hyperparameter optimization technique that selects candidates randomly from a defined search space. For each iteration, a set of hyperparameters is randomly sampled from the parameter distributions. The model is trained and evaluated. This random sampling of configurations is repeated for multiple iterations. The optimal hyperparameters are selected based on the iteration that achieved the best performance on a validation set. Unlike grid search, random search does not test all possible combinations but explores the space efficiently. The search space can be defined broadly allowing for a greater coverage of values.
Bias-variance decomposition provides a detailed breakdown of the contribution of bias and variance to the total expected test error of a model. The expected test error can be expressed as the sum of three sources of error - bias, variance and intrinsic noise in the problem. Bias is error due to oversimplifying assumptions in the model. Variance is sensitivity to changes in the training data. Irreducible error is the intrinsic noise in the problem. Understanding this decomposition provides insights into improving model generalization by reducing bias and variance. Techniques like cross-validation and regularization can help find the right balance.
A learning curve represents the validation error trend for a machine learning model when trained on different sizes of training datasets. It plots the amount of training data vs the validation error. As the training set size increases, the validation error initially decreases, reaches a minimum and then increases. The initial part represents underfitting which reduces by adding data. The upwards trend indicates overfitting and provides an estimate of optimal data size. Learning curves help diagnose common modeling issues like high bias, high variance and give a sense of how much performance can improve with more data.
The curse of dimensionality refers to the exponential increase in volume associated with adding extra dimensions. As the number of features or dimensions grows, the amount of data needed to support the model increases exponentially. Having more dimensions increases sparsity of data making it difficult to find patterns. In high dimensions, the data becomes increasingly sparse leading to a higher risk of overfitting. Feature selection, dimensionality reduction and regularization help in mitigating this problem. The curse of dimensionality impacts algorithms differently based on whether they are distance-based, probabilistic or reconstruction-based.
Stratified sampling is a technique used to ensure that the train-test split represents the relative class frequencies in the original dataset. Samples are divided into homogeneous subgroups called strata, and samples are randomly selected proportionally from the stratum. It aims to produce a sample that has approximately the same class proportions as the original set. This helps avoid sampling bias and overrepresentation of a particular class in the subsets. Stratification is commonly used in machine learning before cross-validation or train-test splits to improve model evaluation.
Feature engineering is the process of using domain knowledge to extract features from raw data via data transformations to improve model performance. It involves generating new predictive features by combining existing features, applying mathematical transformations, aggregating values etc. Feature engineering leverages understanding of the data and domain to extract meaningful representations before feeding into machine learning algorithms. It is crucial for challenging problems where good features are key to model performance. Methods include binning, scaling, normalization, polynomial features, aggregations and dimensionality reduction techniques.
Overfitting refers to a model that models the training data too well, capturing noise and details, that it negatively impacts its ability to generalize to new data. It occurs when a model fits the training data too closely. An overfit model has low bias but very high variance. It has memorized the noise and details in the training set to the extent that it negatively impacts its performance on new data. This leads to poor predictions on unseen data though it may have high performance on training data. Overfitting can be mitigated by techniques like regularization, early stopping, and dropout.
Principal component analysis or PCA is a statistical technique to convert possibly correlated variables into linearly uncorrelated variables using an orthogonal transformation. It projects the data onto a new coordinate system such that the greatest variance of the data lies along the first coordinate called the first principal component. Successive coordinates capture decreasing variance. PCA is a dimensionality reduction technique that can be used for visualization, noise filtering, feature extraction and improving model performance by reducing overfitting. PCA is an unsupervised method that finds hidden structure in data and captures the directions of maximum variance.
Data leakage refers to the unintentional leakage of information about the target variable into the training data which can lead to model overfitting. It causes the model to make overly optimistic predictions on new data because some info was leaked into the training data. Data leakage commonly occurs due to flaws in data collection, annotation, splits or transformations. Detecting and avoiding data leakage is crucial for machine learning to avoid creating invalid models that fail in production. Common cases include unintended peeking at the test labels, train-test contamination, and leakage via derived attributes.
Multicollinearity refers to the occurrence of high correlations among predictor variables in a regression model. It results in inflated variances of the coefficient estimates and makes the estimates unstable and difficult to interpret. Strong multicollinearity can negatively impact model performance. It can be detected using correlation analysis, variance inflation factors and eigenvectors. Remedial measures include removing redundant features, dimensionality reduction techniques like PCA and ridge regression. Ridge regression penalizes coefficient magnitude which ameliorates impact of multicollinearity.
AdaBoost (Adaptive Boosting) is a machine learning ensemble meta-algorithm that combines multiple weak learners into a strong learner in an iterative manner. It focuses on training data that was previously misclassified by weak learners. AdaBoost assigns weights to each training instance. The weights are adjusted adaptively in each round based on the error of the weak learner. Examples that are misclassified gain higher weightage. A new weak learner is added that focuses on the misclassified examples. Predictions from all weak learners are combined through a weighted majority vote to produce the final prediction. AdaBoost reduces bias and variance to improve model performance.
XGBoost stands for Extreme Gradient Boosting. It is an implementation of gradient boosted decision trees designed for speed and performance. XGBoost builds the model in a stage-wise fashion like other boosting methods. It introduces regularization hyperparameters to reduce overfitting which is a challenge for other gradient boosting algorithms. It splits the data into subsets and makes optimal split predictions in parallel using all CPU cores. XGBoost provides hyperparameters for tuning and optimization like subsampling, tree complexity, learning rate. The major advantages of XGBoost are faster training speed, higher efficiency, in-built regularization, parallel processing, cache optimization, handling sparse data and missing values.
Feature scaling refers to the rescaling of features to a common scale, often prior to model training. It is performed to handle features with varying scales and ranges. Scaling brings all features to the same level of magnitude. Common scaling methods are min-max scaling to [0,1] range and standardization to unit variance and zero mean. Feature scaling prevents certain features from dominating others due to their greater magnitude. It speeds up computation and optimization of models like gradient descent. Scaling transforms data without distorting differences in the ranges of features.
Mean normalization scales features by subtracting the mean and scaling to the range zero and one. Mean normalization centers data at zero mean and scales the variability to a fixed range. Centering at mean aids in gradient-based optimization. Normalization to unit range handles varied distributions. Mean normalization is useful preprocessing technique for many machine learning algorithms.
Standardization scales the features to have zero mean and unit variance. Standardization ensures standard normal distribution with no effect of mean and variance differences. Most machine learning techniques assume a normal distribution for features. Standardization helps algorithms that weight inputs like linear regression and distance-based models like KNN. It ensures all features are weighted equally.
One-hot encoding transforms categorical variables with N possible values into N binary variables with one value hot or one. Each binary variable indicates presence or absence of category value. One-hot encoding expands categorical variables into indicator variables equal to one or zero to represent each category. It allows representation of categorical data into a format that algorithms can interpret easily. One-hot encoding avoids assumptions of ordinal relationships in categories. It enables using categorical data in mathematical operations by transforming into numeric representations.
Sigmoid squashes real-valued inputs to outputs between zero and one. Used for binary classification.
Tanh squashes real-valued inputs to outputs between minus zero and one
Softmax converts outputs to probability distribution over predicted classes. For multiclass classification.
